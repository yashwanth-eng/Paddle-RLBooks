{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "Value Iteration is a dynamic programming algorithm used to solve Markov Decision Processes (MDPs). It computes the optimal policy and the value function for an agent interacting with an environment. The algorithm iteratively updates the value of each state until it converges to the optimal value function.\n",
    "\n",
    "#### Key Concepts:\n",
    "1. **Value Function (V)**: Represents the maximum expected reward an agent can achieve starting from a given state and following the optimal policy.\n",
    "2. **Bellman Equation**: The core of value iteration, it expresses the relationship between the value of a state and the values of its successor states.\n",
    "\n",
    "#### Algorithm Steps:\n",
    "1. **Initialization**: Start with an arbitrary value function $V(s)$ for all states $s$ (e.g., $V(s) = 0$).\n",
    "2. **Update Rule**: For each state $s$, update its value using:\n",
    "    $$\n",
    "    V(s) = \\max_a \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V(s') \\right]\n",
    "    $$\n",
    "   where:\n",
    "    - $P(s' | s, a)$: Transition probability from state $s$ to $s'$ under action $a$.\n",
    "    - $R(s, a, s')$: Reward received when transitioning from $s$ to $s'$ under action $a$.\n",
    "    - $\\gamma$: Discount factor ($0 \\leq \\gamma < 1$) that prioritizes immediate rewards over future rewards.\n",
    "3. **Convergence**: Repeat the update rule until the value function converges (i.e., the change in $V(s)$ is smaller than a predefined threshold).\n",
    "\n",
    "#### Optimal Policy:\n",
    "Once the value function converges, the optimal policy $\\pi^*(s)$ can be derived as:\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V(s') \\right]\n",
    "$$\n",
    "\n",
    "#### Advantages:\n",
    "- Guarantees convergence to the optimal value function and policy.\n",
    "- Simple and effective for small state and action spaces.\n",
    "\n",
    "#### Limitations:\n",
    "- Computationally expensive for large state or action spaces due to the need to evaluate all states and actions in each iteration.\n",
    "- Requires knowledge of the transition probabilities and rewards, which may not always be available.\n",
    "\n",
    "Value Iteration is widely used in reinforcement learning and decision-making problems where the environment can be modeled as an MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    # Initialize the value function for all states to zero\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # Set the maximum number of iterations and the convergence threshold\n",
    "    max_iterations = 200000\n",
    "    eps = 1e-5\n",
    "    \n",
    "    # Iterate to update the value function\n",
    "    for i in range(max_iterations):\n",
    "        # Make a copy of the current value function to track changes\n",
    "        v_ = np.copy(v)\n",
    "        \n",
    "        # Loop through each state in the environment\n",
    "        for state in range(env.observation_space.n):\n",
    "            # Compute the value of each action by summing over all possible transitions\n",
    "            q = [sum([prob * (reward + gamma * v_[state_]) \n",
    "                      for prob, state_, reward, _ in env.env.P[state][action]]) \n",
    "                 for action in range(env.action_space.n)]\n",
    "            \n",
    "            # Update the value of the current state to the maximum value of all actions\n",
    "            v[state] = max(q)\n",
    "        \n",
    "        # Check for convergence by comparing the change in value function\n",
    "        if (np.sum(np.fabs(v_ - v)) <= eps):\n",
    "            print('Value-iteration converged at iteration %d.' % (i + 1))\n",
    "            break\n",
    "\n",
    "    # Return the optimal value function\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_extraction(env, v, gamma = 1.0):\n",
    "    # Initialize the policy with zeros for all states\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # Loop through each state in the environment\n",
    "    for state in range(env.observation_space.n):\n",
    "        # Initialize an array to store the value of each action\n",
    "        q = np.zeros(env.action_space.n)\n",
    "        \n",
    "        # Loop through each action available in the current state\n",
    "        for action in range(env.action_space.n):\n",
    "            # Compute the value of the action by summing over all possible transitions\n",
    "            for prob, state_, reward, _ in env.env.P[state][action]:\n",
    "                q[action] += prob * (reward + gamma * v[state_])\n",
    "        \n",
    "        # Select the action with the highest value as the optimal action for the current state\n",
    "        policy[state] = np.argmax(q)\n",
    "\n",
    "    # Return the extracted policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    # Reset the environment to the initial state\n",
    "    observation = env.reset()\n",
    "\n",
    "    if isinstance(observation, tuple):\n",
    "        observation = observation[0]  # Extract the initial observation\n",
    "    else:\n",
    "        observation = observation\n",
    "\n",
    "\n",
    "    total_reward = 0  # Initialize the total reward\n",
    "    step_idx = 0  # Initialize the step counter\n",
    "\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()  # Render the environment if specified\n",
    "        # Take the action dictated by the policy for the current state\n",
    "        observation, reward, done, truncated, _ = env.step(int(policy[observation]))\n",
    "        # Accumulate the discounted reward\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1  # Increment the step counter\n",
    "        if done:  # Exit the loop if the episode is finished\n",
    "            break\n",
    "\n",
    "    return total_reward  # Return the total reward for the episode\n",
    "\n",
    "def test_episode(env, policy):\n",
    "    # Reset the environment to the initial state\n",
    "    observation = env.reset()\n",
    "\n",
    "    if isinstance(observation, tuple):\n",
    "        observation = observation[0]  # Extract the initial observation\n",
    "    else:\n",
    "        observation = observation\n",
    "\n",
    "    while True:\n",
    "        env.render()  # Render the environment\n",
    "        # Take the action dictated by the policy for the current state\n",
    "        observation, _, done, truncated, _ = env.step(int(policy[observation]))\n",
    "        if done:  # Exit the loop if the episode is finished\n",
    "            break\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    # Run multiple episodes and calculate the average score\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)  # Return the mean score across episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration 813.\n",
      "Average scores =  1.0\n"
     ]
    }
   ],
   "source": [
    "env_name = 'FrozenLake8x8-v1' \n",
    "env = gym.make(env_name)\n",
    "optimal_v = value_iteration(env, gamma = 1.0)\n",
    "policy = policy_extraction(env, optimal_v, gamma = 1.0)\n",
    "score = evaluate_policy(env, policy, gamma = 1.0)\n",
    "print('Average scores = ', np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashwanth/projects/volunteering/Paddle-RLBooks/venv/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:271: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"FrozenLake8x8-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "test_episode(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 2.],\n",
       "       [0., 0., 0., 0., 2., 3., 3., 2.],\n",
       "       [0., 0., 0., 1., 0., 0., 2., 2.],\n",
       "       [0., 3., 0., 0., 2., 1., 3., 2.],\n",
       "       [0., 0., 0., 1., 3., 0., 0., 2.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 2.],\n",
       "       [0., 1., 0., 0., 1., 2., 1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.reshape(8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class GridWorldEnv(Env):\n",
    "    def __init__(self, grid_size=4, holes=None, mode = None):\n",
    "        super(GridWorldEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.holes = holes if holes else []  # List of hole states\n",
    "        self.observation_space = spaces.Discrete(grid_size * grid_size)  # Total states\n",
    "        self.action_space = spaces.Discrete(4)  # Actions: 0=Left, 1=Down, 2=Right, 3=Up\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        # Define the transition probabilities and rewards\n",
    "        self.P = self._build_transition_probabilities()\n",
    "        self.env = self  # Make the environment accessible via self.env\n",
    "\n",
    "    def _build_transition_probabilities(self):\n",
    "        P = {}\n",
    "        for state in range(self.observation_space.n):\n",
    "            P[state] = {action: [] for action in range(self.action_space.n)}\n",
    "            x, y = divmod(state, self.grid_size)\n",
    "\n",
    "            for action in range(self.action_space.n):\n",
    "                if action == 0:  # Left\n",
    "                    next_x, next_y = x, max(y - 1, 0)\n",
    "                elif action == 1:  # Down\n",
    "                    next_x, next_y = min(x + 1, self.grid_size - 1), y\n",
    "                elif action == 2:  # Right\n",
    "                    next_x, next_y = x, min(y + 1, self.grid_size - 1)\n",
    "                elif action == 3:  # Up\n",
    "                    next_x, next_y = max(x - 1, 0), y\n",
    "\n",
    "                next_state = next_x * self.grid_size + next_y\n",
    "                if next_state in self.holes:\n",
    "                    reward = -1  # Negative reward for falling into a hole\n",
    "                    done = True  # Episode ends if the agent falls into a hole\n",
    "                elif next_state == self.observation_space.n - 1:\n",
    "                    reward = 1  # Positive reward for reaching the goal\n",
    "                    done = True\n",
    "                else:\n",
    "                    reward = 0  # No reward for other states\n",
    "                    done = False\n",
    "\n",
    "                P[state][action].append((1.0, next_state, reward, done))\n",
    "        return P\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        transitions = self.P[self.state][action]\n",
    "        prob, next_state, reward, done = transitions[0]\n",
    "        self.state = next_state\n",
    "\n",
    "        return next_state, reward, done, False, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n",
    "        grid[:] = '.'\n",
    "        for hole in self.holes:\n",
    "            x, y = divmod(hole, self.grid_size)\n",
    "            grid[x, y] = 'H'  # Mark holes\n",
    "        x, y = divmod(self.state, self.grid_size)\n",
    "        grid[x, y] = 'A'  # Mark agent's position\n",
    "        grid[-1, -1] = 'G'  # Mark goal\n",
    "        print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores =  1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = GridWorldEnv(grid_size=4, holes=[5,10,13])\n",
    "optimal_v = value_iteration(env, gamma = 1.0)\n",
    "optimal_policy = policy_extraction(env, optimal_v, gamma = 1.0)\n",
    "score = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
    "print('Average scores = ', np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 1., 1.],\n",
       "       [1., 2., 2., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [2., 2., 2., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy.reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A . . .\n",
      ". H . .\n",
      ". H . .\n",
      ". H . G\n",
      "\n",
      ". A . .\n",
      ". H . .\n",
      ". H . .\n",
      ". H . G\n",
      "\n",
      ". . A .\n",
      ". H . .\n",
      ". H . .\n",
      ". H . G\n",
      "\n",
      ". . . .\n",
      ". H A .\n",
      ". H . .\n",
      ". H . G\n",
      "\n",
      ". . . .\n",
      ". H . A\n",
      ". H . .\n",
      ". H . G\n",
      "\n",
      ". . . .\n",
      ". H . .\n",
      ". H . A\n",
      ". H . G\n",
      "\n",
      "[[2. 2. 1. 1.]\n",
      " [1. 2. 2. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [2. 2. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = GridWorldEnv(grid_size=4, holes=[5, 9, 13], mode='human')\n",
    "test_episode(env, optimal_policy)\n",
    "print(optimal_policy.reshape(env.grid_size, env.grid_size))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

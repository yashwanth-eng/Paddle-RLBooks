{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "Value Iteration is a dynamic programming algorithm used to solve Markov Decision Processes (MDPs). It computes the optimal policy and the value function for an agent interacting with an environment. The algorithm iteratively updates the value of each state until it converges to the optimal value function.\n",
    "\n",
    "#### Key Concepts:\n",
    "1. **Value Function (V)**: Represents the maximum expected reward an agent can achieve starting from a given state and following the optimal policy.\n",
    "2. **Bellman Equation**: The core of value iteration, it expresses the relationship between the value of a state and the values of its successor states.\n",
    "\n",
    "#### Algorithm Steps:\n",
    "1. **Initialization**: Start with an arbitrary value function $V(s)$ for all states $s$ (e.g., $V(s) = 0$).\n",
    "2. **Update Rule**: For each state $s$, update its value using:\n",
    "    $$\n",
    "    V(s) = \\max_a \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V(s') \\right]\n",
    "    $$\n",
    "   where:\n",
    "    - $P(s' | s, a)$: Transition probability from state $s$ to $s'$ under action $a$.\n",
    "    - $R(s, a, s')$: Reward received when transitioning from $s$ to $s'$ under action $a$.\n",
    "    - $\\gamma$: Discount factor ($0 \\leq \\gamma < 1$) that prioritizes immediate rewards over future rewards.\n",
    "3. **Convergence**: Repeat the update rule until the value function converges (i.e., the change in $V(s)$ is smaller than a predefined threshold).\n",
    "\n",
    "#### Optimal Policy:\n",
    "Once the value function converges, the optimal policy $\\pi^*(s)$ can be derived as:\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V(s') \\right]\n",
    "$$\n",
    "\n",
    "#### Advantages:\n",
    "- Guarantees convergence to the optimal value function and policy.\n",
    "- Simple and effective for small state and action spaces.\n",
    "\n",
    "#### Limitations:\n",
    "- Computationally expensive for large state or action spaces due to the need to evaluate all states and actions in each iteration.\n",
    "- Requires knowledge of the transition probabilities and rewards, which may not always be available.\n",
    "\n",
    "Value Iteration is widely used in reinforcement learning and decision-making problems where the environment can be modeled as an MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma = 1.0):\n",
    "    # Initialize the value function for all states to zero\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # Set the maximum number of iterations and the convergence threshold\n",
    "    max_iterations = 200000\n",
    "    eps = 1e-5\n",
    "    \n",
    "    # Iterate to update the value function\n",
    "    for i in range(max_iterations):\n",
    "        # Make a copy of the current value function to track changes\n",
    "        v_ = np.copy(v)\n",
    "        \n",
    "        # Loop through each state in the environment\n",
    "        for state in range(env.observation_space.n):\n",
    "            # Compute the value of each action by summing over all possible transitions\n",
    "            q = [sum([prob * (reward + gamma * v_[state_]) \n",
    "                      for prob, state_, reward, _ in env.env.P[state][action]]) \n",
    "                 for action in range(env.action_space.n)]\n",
    "            \n",
    "            # Update the value of the current state to the maximum value of all actions\n",
    "            v[state] = max(q)\n",
    "        \n",
    "        # Check for convergence by comparing the change in value function\n",
    "        if (np.sum(np.fabs(v_ - v)) <= eps):\n",
    "            print('Value-iteration converged at iteration %d.' % (i + 1))\n",
    "            break\n",
    "\n",
    "    # Return the optimal value function\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_extraction(env, v, gamma = 1.0):\n",
    "    # Initialize the policy with zeros for all states\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # Loop through each state in the environment\n",
    "    for state in range(env.observation_space.n):\n",
    "        # Initialize an array to store the value of each action\n",
    "        q = np.zeros(env.action_space.n)\n",
    "        \n",
    "        # Loop through each action available in the current state\n",
    "        for action in range(env.action_space.n):\n",
    "            # Compute the value of the action by summing over all possible transitions\n",
    "            for prob, state_, reward, _ in env.env.P[state][action]:\n",
    "                q[action] += prob * (reward + gamma * v[state_])\n",
    "        \n",
    "        # Select the action with the highest value as the optimal action for the current state\n",
    "        policy[state] = np.argmax(q)\n",
    "\n",
    "    # Return the extracted policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration 813.\n",
      "Average scores =  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashwanth/projects/volunteering/Paddle-RLBooks/venv/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:271: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"FrozenLake8x8-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    # Reset the environment to the initial state\n",
    "    observation = env.reset()\n",
    "    observation = observation[0]  # Extract the initial observation\n",
    "    total_reward = 0  # Initialize the total reward\n",
    "    step_idx = 0  # Initialize the step counter\n",
    "\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()  # Render the environment if specified\n",
    "        # Take the action dictated by the policy for the current state\n",
    "        observation, reward, done, truncated, _ = env.step(int(policy[observation]))\n",
    "        # Accumulate the discounted reward\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1  # Increment the step counter\n",
    "        if done:  # Exit the loop if the episode is finished\n",
    "            break\n",
    "\n",
    "    return total_reward  # Return the total reward for the episode\n",
    "\n",
    "def test_episode(env, policy):\n",
    "    # Reset the environment to the initial state\n",
    "    observation = env.reset()\n",
    "    observation = observation[0]  # Extract the initial observation\n",
    "\n",
    "    while True:\n",
    "        env.render()  # Render the environment\n",
    "        # Take the action dictated by the policy for the current state\n",
    "        observation, _, done, truncated, _ = env.step(int(policy[observation]))\n",
    "        if done:  # Exit the loop if the episode is finished\n",
    "            break\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    # Run multiple episodes and calculate the average score\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)  # Return the mean score across episodes\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name = 'FrozenLake8x8-v1' \n",
    "    env = gym.make(env_name)\n",
    "    optimal_v = value_iteration(env, gamma = 1.0)\n",
    "    policy = policy_extraction(env, optimal_v, gamma = 1.0)\n",
    "    score = evaluate_policy(env, policy, gamma = 1.0)\n",
    "    print('Average scores = ', np.mean(score))\n",
    "    test_episode(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(env_name, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mtest_episode\u001b[0;34m(env, policy)\u001b[0m\n\u001b[1;32m     24\u001b[0m observation \u001b[38;5;241m=\u001b[39m observation[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Extract the initial observation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Render the environment\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Take the action dictated by the policy for the current state\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     observation, _, done, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mint\u001b[39m(policy[observation]))\n",
      "File \u001b[0;32m~/projects/volunteering/Paddle-RLBooks/venv/lib/python3.10/site-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/volunteering/Paddle-RLBooks/venv/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/volunteering/Paddle-RLBooks/venv/lib/python3.10/site-packages/gym/wrappers/env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/volunteering/Paddle-RLBooks/venv/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:279\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/volunteering/Paddle-RLBooks/venv/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:373\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    371\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    372\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    376\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name, render_mode='human')\n",
    "test_episode(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99998899, 0.99998947, 0.99999016, 0.99999092, 0.99999168,\n",
       "        0.99999242, 0.99999307, 0.99999355],\n",
       "       [0.99998887, 0.99998923, 0.99998983, 0.99999055, 0.99999132,\n",
       "        0.99999209, 0.99999289, 0.99999383],\n",
       "       [0.99997859, 0.97818182, 0.92641358, 0.        , 0.85660943,\n",
       "        0.94622381, 0.98207026, 0.99999438],\n",
       "       [0.99996925, 0.93457851, 0.80106982, 0.47489431, 0.62361351,\n",
       "        0.        , 0.94467201, 0.99999518],\n",
       "       [0.99996124, 0.8255856 , 0.54221824, 0.        , 0.53933714,\n",
       "        0.61118455, 0.85195085, 0.99999619],\n",
       "       [0.99995491, 0.        , 0.        , 0.16803797, 0.3832136 ,\n",
       "        0.44226587, 0.        , 0.99999736],\n",
       "       [0.99995053, 0.        , 0.19466348, 0.12090042, 0.        ,\n",
       "        0.33239961, 0.        , 0.99999865],\n",
       "       [0.99994829, 0.73151853, 0.46309046, 0.        , 0.27746651,\n",
       "        0.55493304, 0.77746651, 0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_v.reshape(8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [3., 3., 3., 3., 3., 3., 3., 2.],\n",
       "       [0., 0., 0., 0., 2., 3., 3., 2.],\n",
       "       [0., 0., 0., 1., 0., 0., 2., 2.],\n",
       "       [0., 3., 0., 0., 2., 1., 3., 2.],\n",
       "       [0., 0., 0., 1., 3., 0., 0., 2.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 2.],\n",
       "       [0., 1., 0., 0., 1., 2., 1., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.reshape(8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration Algorithm\n",
    "\n",
    "Policy Iteration is a method used to solve Markov Decision Processes (MDPs). It alternates between two steps: policy evaluation and policy improvement, iteratively improving the policy until it converges to the optimal policy.\n",
    "\n",
    "### Steps of the Policy Iteration Algorithm:\n",
    "\n",
    "1. **Initialization**:\n",
    "    - Start with an arbitrary policy $\\pi$.\n",
    "    - Initialize the value function $V(s)$ for all states $s$.\n",
    "\n",
    "2. **Policy Evaluation**:\n",
    "    - Compute the value function $V^\\pi(s) $ for the current policy $\\pi$ by solving the Bellman equation:  \n",
    "        $$  \n",
    "        V^\\pi(s) = \\sum_{s'} P(s'|s, \\pi(s)) \\left[ R(s, \\pi(s), s') + \\gamma V^\\pi(s') \\right]  \n",
    "        $$  \n",
    "        where:  \n",
    "        - $P(s'|s, a)$: Transition probability.  \n",
    "        - $R(s, a, s')$: Reward for transitioning from state \\( s \\) to \\( s' \\) using action \\( a \\).  \n",
    "        - $\\gamma$: Discount factor. \n",
    "    - Update the policy by acting greedily with respect to the value function:\n",
    "      $$\n",
    "      \\pi'(s) = \\arg\\max_a \\sum_{s'} P(s'|s, a) \\left[ R(s, a, s') + \\gamma V^\\pi(s') \\right]\n",
    "      $$\n",
    "\n",
    "4. **Convergence**:\n",
    "    - Repeat the policy evaluation and policy improvement steps until the policy no longer changes (i.e., it converges to the optimal policy $\\pi^*$).\n",
    "\n",
    "Policy Iteration is guaranteed to converge to the optimal policy in a finite number of iterations for finite MDPs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, v, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the value function for a given policy.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment object (e.g., OpenAI Gym environment) that provides the transition dynamics.\n",
    "    - policy: A numpy array representing the current policy, where policy[state] gives the action to take in that state.\n",
    "    - v: A numpy array representing the current value function, where v[state] is the value of the state.\n",
    "    - gamma: The discount factor (default is 1.0), which determines the importance of future rewards.\n",
    "\n",
    "    Returns:\n",
    "    - v: The updated value function after policy evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the current value function to track updates\n",
    "    v_ = np.copy(v)\n",
    "    \n",
    "    # Iterate over all states in the environment\n",
    "    for state in range(env.observation_space.n):\n",
    "        # Get the action dictated by the current policy for this state\n",
    "        policy_action = policy[state]\n",
    "        \n",
    "        # Update the value of the current state using the Bellman equation\n",
    "        # Sum over all possible transitions (probability, next state, reward, done)\n",
    "        v[state] = sum([\n",
    "            prob * (reward + gamma * v_[state_]) \n",
    "            for prob, state_, reward, _ in env.env.P[state][policy_action]\n",
    "        ])\n",
    "        \n",
    "    # Return the final value function\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "def policy_improvement(env, v, gamma=1.0):\n",
    "    # Initialize a new policy with zeros for all states\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # Iterate over all states in the environment\n",
    "    for state in range(env.observation_space.n):\n",
    "        # Initialize an array to store the action-value function (Q-values) for all actions\n",
    "        q = np.zeros(env.action_space.n)\n",
    "        \n",
    "        # Iterate over all possible actions for the current state\n",
    "        for action in range(env.action_space.n):\n",
    "            # Compute the Q-value for the current action by summing over all possible transitions\n",
    "            # (probability, next state, reward, done)\n",
    "            q[action] = sum([\n",
    "                prob * (reward + gamma * v[state_]) \n",
    "                for prob, state_, reward, _ in env.env.P[state][action]\n",
    "            ])\n",
    "        \n",
    "        # Select the action that maximizes the Q-value and update the policy for the current state\n",
    "        policy[state] = np.argmax(q)\n",
    "    \n",
    "    # Return the improved policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "\n",
    "    # Initialize the value function for all states to 0\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "\n",
    "    # Initialize policy randomly. \n",
    "    policy = np.random.choice(env.action_space.n, size=(env.observation_space.n))\n",
    "    max_iterations = 20000\n",
    "    gamma = 1.0\n",
    "    for i in range(max_iterations):\n",
    "        new_v = policy_evaluation(env, policy, v, gamma)\n",
    "        new_policy = policy_improvement(env, new_v, gamma)\n",
    "        #print(new_policy.reshape(8,8))\n",
    "        # if (np.all(policy == new_policy)):\n",
    "            # print('Policy-Iteration converged at step %d.' % (i + 1))\n",
    "\n",
    "\n",
    "            # print(new_v.reshape(8,8))\n",
    "         \n",
    "        policy = np.copy(new_policy)\n",
    "        v_ = np.copy(new_v)\n",
    "    return policy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Testing the Policy Iteration Algorithm on FrozenLake Environment\n",
    "\n",
    "The FrozenLake environment is a classic reinforcement learning problem where the agent must navigate a grid world to reach the goal while avoiding holes. The environment is stochastic, meaning the agent's actions may not always lead to the intended outcome.\n",
    "\n",
    "### Environment Details:\n",
    "- **Environment Name**: `FrozenLake8x8-v1`\n",
    "- **Grid Size**: 8x8\n",
    "- **Objective**: Navigate from the starting point to the goal while avoiding holes.\n",
    "- **Actions**: Discrete actions represented by numbers:\n",
    "    - `0`: Left\n",
    "    - `1`: Down\n",
    "    - `2`: Right\n",
    "    - `3`: Up\n",
    "- **Rewards**: A reward of 1 is given for reaching the goal; otherwise, the reward is 0.\n",
    "\n",
    "### Testing Workflow:\n",
    "1. **Policy Iteration**:\n",
    "    - Use the Policy Iteration algorithm to compute the optimal policy for the FrozenLake environment.\n",
    "    - The algorithm alternates between policy evaluation and policy improvement until convergence.\n",
    "\n",
    "2. **Evaluation**:\n",
    "    - Evaluate the optimal policy by running multiple episodes and calculating the average score.\n",
    "\n",
    "3. **Visualization**:\n",
    "    - Test the optimal policy by running a single episode and rendering the environment to visualize the agent's behavior.\n",
    "\n",
    "### Results:\n",
    "- **Optimal Policy**: The computed policy is stored in the variable `optimal_policy`.\n",
    "- **Average Score**: The average score over multiple episodes is stored in the variable `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\"\n",
    "    Runs a single episode using the given policy.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment object (e.g., OpenAI Gym environment).\n",
    "    - policy: A numpy array representing the policy, where policy[state] gives the action to take in that state.\n",
    "    - gamma: The discount factor (default is 1.0), which determines the importance of future rewards.\n",
    "    - render: A boolean flag to render the environment during the episode (default is False).\n",
    "\n",
    "    Returns:\n",
    "    - total_reward: The total discounted reward obtained during the episode.\n",
    "    \"\"\"\n",
    "    observation = env.reset()  # Reset the environment to the initial state.\n",
    "    total_reward = 0  # Initialize the total reward.\n",
    "    step_idx = 0  # Initialize the step counter.\n",
    "\n",
    "    if isinstance(observation, tuple):\n",
    "        observation = observation[0]  # Extract the initial state from the reset output.\n",
    "    else:\n",
    "        observation = observation\n",
    "\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()  # Render the environment if the render flag is True.\n",
    "\n",
    "        action = int(policy[observation])  # Select the action based on the policy.\n",
    "        try:\n",
    "            # Take the action and observe the next state, reward, and done flag.\n",
    "            observation, reward, done, truncated, _ = env.step(action)\n",
    "            # Accumulate the discounted reward.\n",
    "            total_reward += (gamma ** step_idx * reward)\n",
    "            step_idx += 1  # Increment the step counter.\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions that occur during the episode.\n",
    "            print(f\"An error occurred during the episode: {e}\")\n",
    "            break\n",
    "        if done:  # Exit the loop if the episode is done.\n",
    "            break\n",
    "\n",
    "    return total_reward  # Return the total discounted reward.\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    \"\"\"\n",
    "    Evaluates the given policy by running multiple episodes and averaging the total rewards.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment object (e.g., OpenAI Gym environment).\n",
    "    - policy: A numpy array representing the policy, where policy[state] gives the action to take in that state.\n",
    "    - gamma: The discount factor (default is 1.0), which determines the importance of future rewards.\n",
    "    - n: The number of episodes to run for evaluation (default is 100).\n",
    "\n",
    "    Returns:\n",
    "    - The average total discounted reward over n episodes.\n",
    "    \"\"\"\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]  # Run n episodes and collect scores.\n",
    "    return np.mean(scores)  # Return the average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_episode(env, policy):\n",
    "    \"\"\"\n",
    "    Runs a single episode to test the policy and renders the environment.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment object (e.g., OpenAI Gym environment).\n",
    "    - policy: A numpy array representing the policy, where policy[state] gives the action to take in that state.\n",
    "    \"\"\"\n",
    "    observation = env.reset()  # Reset the environment to the initial state.\n",
    "    if isinstance(observation, tuple):\n",
    "        observation = observation[0]  # Extract the initial state from the reset output.\n",
    "    else:\n",
    "        observation = observation\n",
    "        \n",
    "    while True:\n",
    "        env.render()  # Render the environment.\n",
    "        # Take the action based on the policy and observe the next state and done flag.\n",
    "        observation, _, done, truncated, _ = env.step(int(policy[observation]))\n",
    "        if done:  # Exit the loop if the episode is done.\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores =  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "env_name  = 'FrozenLake8x8-v1' # 'FrozenLake-v0'\n",
    "env = gym.make(env_name)\n",
    "optimal_policy = policy_iteration(env, gamma = 1.0)\n",
    "score = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
    "print('Average scores = ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(env_name, render_mode='human')\n",
    "# est_episode(env, optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 2. 1. 2. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 2.]\n",
      " [0. 0. 0. 0. 2. 3. 3. 2.]\n",
      " [0. 0. 0. 1. 0. 0. 2. 2.]\n",
      " [0. 3. 0. 0. 2. 1. 3. 2.]\n",
      " [0. 0. 0. 1. 3. 0. 0. 2.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 2.]\n",
      " [0. 1. 0. 0. 1. 2. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy.reshape(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(Env):\n",
    "    def __init__(self, grid_size=4, holes=None, mode = None):\n",
    "        super(GridWorldEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.holes = holes if holes else []  # List of hole states\n",
    "        self.observation_space = spaces.Discrete(grid_size * grid_size)  # Total states\n",
    "        self.action_space = spaces.Discrete(4)  # Actions: 0=Left, 1=Down, 2=Right, 3=Up\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        # Define the transition probabilities and rewards\n",
    "        self.P = self._build_transition_probabilities()\n",
    "        self.env = self  # Make the environment accessible via self.env\n",
    "\n",
    "    def _build_transition_probabilities(self):\n",
    "        P = {}\n",
    "        for state in range(self.observation_space.n):\n",
    "            P[state] = {action: [] for action in range(self.action_space.n)}\n",
    "            x, y = divmod(state, self.grid_size)\n",
    "\n",
    "            for action in range(self.action_space.n):\n",
    "                if action == 0:  # Left\n",
    "                    next_x, next_y = x, max(y - 1, 0)\n",
    "                elif action == 1:  # Down\n",
    "                    next_x, next_y = min(x + 1, self.grid_size - 1), y\n",
    "                elif action == 2:  # Right\n",
    "                    next_x, next_y = x, min(y + 1, self.grid_size - 1)\n",
    "                elif action == 3:  # Up\n",
    "                    next_x, next_y = max(x - 1, 0), y\n",
    "\n",
    "                next_state = next_x * self.grid_size + next_y\n",
    "                if next_state in self.holes:\n",
    "                    reward = -1  # Negative reward for falling into a hole\n",
    "                    done = True  # Episode ends if the agent falls into a hole\n",
    "                elif next_state == self.observation_space.n - 1:\n",
    "                    reward = 1  # Positive reward for reaching the goal\n",
    "                    done = True\n",
    "                else:\n",
    "                    reward = 0  # No reward for other states\n",
    "                    done = False\n",
    "\n",
    "                P[state][action].append((1.0, next_state, reward, done))\n",
    "        return P\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        transitions = self.P[self.state][action]\n",
    "        prob, next_state, reward, done = transitions[0]\n",
    "        self.state = next_state\n",
    "\n",
    "        return next_state, reward, done, False, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n",
    "        grid[:] = '.'\n",
    "        for hole in self.holes:\n",
    "            x, y = divmod(hole, self.grid_size)\n",
    "            grid[x, y] = 'H'  # Mark holes\n",
    "        x, y = divmod(self.state, self.grid_size)\n",
    "        grid[x, y] = 'A'  # Mark agent's position\n",
    "        grid[-1, -1] = 'G'  # Mark goal\n",
    "        print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = GridWorldEnv(grid_size=4, holes=[5, 9, 13])\n",
    "optimal_policy = policy_iteration(env, gamma = 1.0)\n",
    "score = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
    "print('Average scores = ', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 1., 1.],\n",
       "       [1., 2., 1., 1.],\n",
       "       [1., 2., 1., 1.],\n",
       "       [2., 2., 2., 1.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy.reshape(env.grid_size, env.grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A . . .\n",
      ". H . .\n",
      ". H . .\n",
      ". H . G\n",
      "\n",
      ". A . .\n",
      ". H . .\n",
      ". H . .\n",
      ". H . G\n",
      "\n",
      ". . A .\n",
      ". H . .\n",
      ". H . .\n",
      ". H . G\n",
      "\n",
      ". . . .\n",
      ". H A .\n",
      ". H . .\n",
      ". H . G\n",
      "\n",
      ". . . .\n",
      ". H . .\n",
      ". H A .\n",
      ". H . G\n",
      "\n",
      ". . . .\n",
      ". H . .\n",
      ". H . .\n",
      ". H A G\n",
      "\n",
      "[[2. 2. 1. 1.]\n",
      " [1. 2. 1. 1.]\n",
      " [1. 2. 1. 1.]\n",
      " [2. 2. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = GridWorldEnv(grid_size=4, holes=[5, 9, 13], mode='human')\n",
    "test_episode(env, optimal_policy)\n",
    "print(optimal_policy.reshape(env.grid_size, env.grid_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

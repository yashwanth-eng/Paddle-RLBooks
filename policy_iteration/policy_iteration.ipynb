{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration Algorithm\n",
    "\n",
    "Policy Iteration is a method used to solve Markov Decision Processes (MDPs). It alternates between two steps: policy evaluation and policy improvement, iteratively improving the policy until it converges to the optimal policy.\n",
    "\n",
    "### Steps of the Policy Iteration Algorithm:\n",
    "\n",
    "1. **Initialization**:\n",
    "    - Start with an arbitrary policy $\\pi$.\n",
    "    - Initialize the value function $V(s)$ for all states $s$.\n",
    "\n",
    "2. **Policy Evaluation**:\n",
    "    - Compute the value function $V^\\pi(s) $ for the current policy $\\pi$ by solving the Bellman equation:  \n",
    "        $$  \n",
    "        V^\\pi(s) = \\sum_{s'} P(s'|s, \\pi(s)) \\left[ R(s, \\pi(s), s') + \\gamma V^\\pi(s') \\right]  \n",
    "        $$  \n",
    "        where:  \n",
    "        - $P(s'|s, a)$: Transition probability.  \n",
    "        - $R(s, a, s')$: Reward for transitioning from state \\( s \\) to \\( s' \\) using action \\( a \\).  \n",
    "        - $\\gamma$: Discount factor. \n",
    "    - Update the policy by acting greedily with respect to the value function:\n",
    "      $$\n",
    "      \\pi'(s) = \\arg\\max_a \\sum_{s'} P(s'|s, a) \\left[ R(s, a, s') + \\gamma V^\\pi(s') \\right]\n",
    "      $$\n",
    "\n",
    "4. **Convergence**:\n",
    "    - Repeat the policy evaluation and policy improvement steps until the policy no longer changes (i.e., it converges to the optimal policy $\\pi^*$).\n",
    "\n",
    "Policy Iteration is guaranteed to converge to the optimal policy in a finite number of iterations for finite MDPs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, v, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the value function for a given policy.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment object (e.g., OpenAI Gym environment) that provides the transition dynamics.\n",
    "    - policy: A numpy array representing the current policy, where policy[state] gives the action to take in that state.\n",
    "    - v: A numpy array representing the current value function, where v[state] is the value of the state.\n",
    "    - gamma: The discount factor (default is 1.0), which determines the importance of future rewards.\n",
    "\n",
    "    Returns:\n",
    "    - v: The updated value function after policy evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the current value function to track updates\n",
    "    v_ = np.copy(v)\n",
    "    \n",
    "    # Iterate over all states in the environment\n",
    "    for state in range(env.observation_space.n):\n",
    "        # Get the action dictated by the current policy for this state\n",
    "        policy_action = policy[state]\n",
    "        \n",
    "        # Update the value of the current state using the Bellman equation\n",
    "        # Sum over all possible transitions (probability, next state, reward, done)\n",
    "        v[state] = sum([\n",
    "            prob * (reward + gamma * v_[state_]) \n",
    "            for prob, state_, reward, _ in env.env.P[state][policy_action]\n",
    "        ])\n",
    "        \n",
    "    # Return the final value function\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "def policy_improvement(env, v, gamma=1.0):\n",
    "    # Initialize a new policy with zeros for all states\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # Iterate over all states in the environment\n",
    "    for state in range(env.observation_space.n):\n",
    "        # Initialize an array to store the action-value function (Q-values) for all actions\n",
    "        q = np.zeros(env.action_space.n)\n",
    "        \n",
    "        # Iterate over all possible actions for the current state\n",
    "        for action in range(env.action_space.n):\n",
    "            # Compute the Q-value for the current action by summing over all possible transitions\n",
    "            # (probability, next state, reward, done)\n",
    "            q[action] = sum([\n",
    "                prob * (reward + gamma * v[state_]) \n",
    "                for prob, state_, reward, _ in env.env.P[state][action]\n",
    "            ])\n",
    "        \n",
    "        # Select the action that maximizes the Q-value and update the policy for the current state\n",
    "        policy[state] = np.argmax(q)\n",
    "    \n",
    "    # Return the improved policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "\n",
    "    # Initialize the value function for all states to 0\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "\n",
    "    # Initialize policy randomly. \n",
    "    policy = np.random.choice(env.action_space.n, size=(env.observation_space.n))\n",
    "    max_iterations = 200000\n",
    "    gamma = 1.0\n",
    "    for i in range(max_iterations):\n",
    "        new_v = policy_evaluation(env, policy, v, gamma)\n",
    "        new_policy = policy_improvement(env, new_v, gamma)\n",
    "        #print(new_policy.reshape(8,8))\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print('Policy-Iteration converged at step %d.' % (i + 1))\n",
    "\n",
    "\n",
    "            print(new_v.reshape(8,8))\n",
    "            break\n",
    "        policy = np.copy(new_policy)\n",
    "        v_ = np.copy(new_v)\n",
    "    return policy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Testing the Policy Iteration Algorithm on FrozenLake Environment\n",
    "\n",
    "The FrozenLake environment is a classic reinforcement learning problem where the agent must navigate a grid world to reach the goal while avoiding holes. The environment is stochastic, meaning the agent's actions may not always lead to the intended outcome.\n",
    "\n",
    "### Environment Details:\n",
    "- **Environment Name**: `FrozenLake8x8-v1`\n",
    "- **Grid Size**: 8x8\n",
    "- **Objective**: Navigate from the starting point to the goal while avoiding holes.\n",
    "- **Actions**: Discrete actions represented by numbers:\n",
    "    - `0`: Left\n",
    "    - `1`: Down\n",
    "    - `2`: Right\n",
    "    - `3`: Up\n",
    "- **Rewards**: A reward of 1 is given for reaching the goal; otherwise, the reward is 0.\n",
    "\n",
    "### Testing Workflow:\n",
    "1. **Policy Iteration**:\n",
    "    - Use the Policy Iteration algorithm to compute the optimal policy for the FrozenLake environment.\n",
    "    - The algorithm alternates between policy evaluation and policy improvement until convergence.\n",
    "\n",
    "2. **Evaluation**:\n",
    "    - Evaluate the optimal policy by running multiple episodes and calculating the average score.\n",
    "\n",
    "3. **Visualization**:\n",
    "    - Test the optimal policy by running a single episode and rendering the environment to visualize the agent's behavior.\n",
    "\n",
    "### Results:\n",
    "- **Optimal Policy**: The computed policy is stored in the variable `optimal_policy`.\n",
    "- **Average Score**: The average score over multiple episodes is stored in the variable `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\"\n",
    "    Runs a single episode using the given policy.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment object (e.g., OpenAI Gym environment).\n",
    "    - policy: A numpy array representing the policy, where policy[state] gives the action to take in that state.\n",
    "    - gamma: The discount factor (default is 1.0), which determines the importance of future rewards.\n",
    "    - render: A boolean flag to render the environment during the episode (default is False).\n",
    "\n",
    "    Returns:\n",
    "    - total_reward: The total discounted reward obtained during the episode.\n",
    "    \"\"\"\n",
    "    observation = env.reset()  # Reset the environment to the initial state.\n",
    "    total_reward = 0  # Initialize the total reward.\n",
    "    step_idx = 0  # Initialize the step counter.\n",
    "\n",
    "    observation = observation[0]  # Extract the initial state from the reset output.\n",
    "\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()  # Render the environment if the render flag is True.\n",
    "\n",
    "        action = int(policy[observation])  # Select the action based on the policy.\n",
    "        try:\n",
    "            # Take the action and observe the next state, reward, and done flag.\n",
    "            observation, reward, done, truncated, _ = env.step(action)\n",
    "            # Accumulate the discounted reward.\n",
    "            total_reward += (gamma ** step_idx * reward)\n",
    "            step_idx += 1  # Increment the step counter.\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions that occur during the episode.\n",
    "            print(f\"An error occurred during the episode: {e}\")\n",
    "            break\n",
    "        if done:  # Exit the loop if the episode is done.\n",
    "            break\n",
    "\n",
    "    return total_reward  # Return the total discounted reward.\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    \"\"\"\n",
    "    Evaluates the given policy by running multiple episodes and averaging the total rewards.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment object (e.g., OpenAI Gym environment).\n",
    "    - policy: A numpy array representing the policy, where policy[state] gives the action to take in that state.\n",
    "    - gamma: The discount factor (default is 1.0), which determines the importance of future rewards.\n",
    "    - n: The number of episodes to run for evaluation (default is 100).\n",
    "\n",
    "    Returns:\n",
    "    - The average total discounted reward over n episodes.\n",
    "    \"\"\"\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]  # Run n episodes and collect scores.\n",
    "    return np.mean(scores)  # Return the average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_episode(env, policy):\n",
    "    \"\"\"\n",
    "    Runs a single episode to test the policy and renders the environment.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment object (e.g., OpenAI Gym environment).\n",
    "    - policy: A numpy array representing the policy, where policy[state] gives the action to take in that state.\n",
    "    \"\"\"\n",
    "    observation = env.reset()  # Reset the environment to the initial state.\n",
    "    observation = observation[0]  # Extract the initial state from the reset output.\n",
    "    while True:\n",
    "        env.render()  # Render the environment.\n",
    "        # Take the action based on the policy and observe the next state and done flag.\n",
    "        observation, _, done, truncated, _ = env.step(int(policy[observation]))\n",
    "        if done:  # Exit the loop if the episode is done.\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 14.\n",
      "[[2.23710419e-05 1.50115964e-04 8.24174273e-04 3.11605616e-03\n",
      "  9.23296806e-03 1.89907148e-02 3.18189392e-02 3.91687673e-02]\n",
      " [4.74600609e-05 2.48381288e-04 9.66345381e-04 3.84761014e-03\n",
      "  1.36333729e-02 2.89550277e-02 5.46041590e-02 7.24123447e-02]\n",
      " [9.97288504e-05 2.66779902e-04 6.99147329e-04 0.00000000e+00\n",
      "  1.93712316e-02 3.90834647e-02 9.70311537e-02 1.32345202e-01]\n",
      " [1.54924692e-04 5.73493159e-04 1.85826001e-03 6.41400770e-03\n",
      "  2.17022105e-02 0.00000000e+00 1.49852320e-01 2.19497137e-01]\n",
      " [9.36656708e-05 2.29146373e-04 5.17042866e-04 0.00000000e+00\n",
      "  5.34253933e-02 1.04591939e-01 1.84205041e-01 3.44963766e-01]\n",
      " [1.67260127e-05 0.00000000e+00 0.00000000e+00 1.76505430e-02\n",
      "  5.37816574e-02 1.08324139e-01 0.00000000e+00 5.25233804e-01]\n",
      " [1.75623133e-05 0.00000000e+00 1.73636919e-03 5.51519360e-03\n",
      "  0.00000000e+00 1.88091539e-01 0.00000000e+00 7.50029741e-01]\n",
      " [7.48489066e-05 2.27264697e-04 6.67158830e-04 0.00000000e+00\n",
      "  2.29612611e-01 4.71167804e-01 7.29612506e-01 0.00000000e+00]]\n",
      "Average scores =  0.56\n"
     ]
    }
   ],
   "source": [
    "env_name  = 'FrozenLake8x8-v1' # 'FrozenLake-v0'\n",
    "env = gym.make(env_name)\n",
    "optimal_policy = policy_iteration(env, gamma = 1.0)\n",
    "score = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
    "print('Average scores = ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(env_name, render_mode='human')\n",
    "# test_episode(env, optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [1. 2. 2. 3. 2. 2. 2. 1.]\n",
      " [1. 2. 0. 0. 2. 3. 2. 1.]\n",
      " [3. 2. 3. 1. 0. 0. 2. 1.]\n",
      " [3. 3. 0. 0. 2. 1. 3. 2.]\n",
      " [0. 0. 0. 1. 3. 0. 0. 2.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 2.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy.reshape(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
